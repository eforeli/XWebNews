#!/usr/bin/env python3
"""
æ¯æ—¥å…¨è¦†è“‹çˆ¬èŸ² - æ¯å¤©çˆ¬å–æ‰€æœ‰7å€‹Web3è³½é“
ä½¿ç”¨åˆ†æ™‚æ®µç­–ç•¥é¿é–‹APIé™åˆ¶
"""

import tweepy
import json
import time
import csv
from datetime import datetime, timedelta
from typing import List, Dict, Any
import logging
import random
import os

class FullCoverageWeb3Crawler:
    def __init__(self, bearer_token: str):
        """æ¯æ—¥å…¨è¦†è“‹çˆ¬èŸ² - æ™ºèƒ½åˆ†æ™‚æ®µçˆ¬å–æ‰€æœ‰è³½é“"""
        self.client = tweepy.Client(bearer_token=bearer_token)
        self.setup_logging()
        
        # 7å€‹Web3è³½é“ - æ¯å€‹è³½é“ç²¾é¸é—œéµå­—
        self.web3_categories = {
            "DeFi": "DeFi",
            "Layer1_Layer2": "Ethereum", 
            "NFT_GameFi": "NFT",
            "AI_Crypto": "AI",
            "RWA": "RWA",
            "Meme_Coins": "DOGE",
            "Infrastructure": "Chainlink"
        }

    def setup_logging(self):
        """è¨­ç½®æ—¥èªŒ"""
        timestamp = datetime.now().strftime("%Y%m%d")
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'full_coverage_{timestamp}.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def crawl_single_category_safe(self, category: str, keyword: str, target_tweets: int = 15) -> List[Dict[str, Any]]:
        """å®‰å…¨çˆ¬å–å–®ä¸€è³½é“ - åŒ…å«éŒ¯èª¤è™•ç†å’Œé‡è©¦"""
        
        tweets_data = []
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                self.logger.info(f"ğŸ¯ çˆ¬å– {category} (å˜—è©¦ {attempt + 1}/{max_retries})")
                
                query = f"{keyword} -is:retweet lang:en"
                
                response = self.client.search_recent_tweets(
                    query=query,
                    tweet_fields=['created_at', 'author_id', 'public_metrics'],
                    user_fields=['username', 'verified'],
                    expansions=['author_id'],
                    max_results=min(target_tweets + 5, 100)  # å¤šæŠ“ä¸€äº›ä»¥å‚™ç¯©é¸
                )
                
                if not response or not response.data:
                    self.logger.warning(f"   âš ï¸ {category}: ç„¡æ¨æ–‡çµæœ")
                    if attempt < max_retries - 1:
                        time.sleep(30)  # ç­‰å¾…30ç§’å¾Œé‡è©¦
                        continue
                    return []
                
                # è™•ç†ç”¨æˆ¶ä¿¡æ¯
                users = {}
                if hasattr(response, 'includes') and response.includes and 'users' in response.includes:
                    users = {user.id: user for user in response.includes['users']}
                
                # è™•ç†æ¨æ–‡
                for tweet in response.data:
                    author = users.get(tweet.author_id)
                    metrics = tweet.public_metrics or {}
                    
                    # è¨ˆç®—äº’å‹•åˆ†æ•¸
                    engagement_score = (
                        metrics.get('like_count', 0) * 1 +
                        metrics.get('retweet_count', 0) * 2 +
                        metrics.get('reply_count', 0) * 0.5
                    )
                    
                    tweet_data = {
                        'category': category,
                        'tweet_id': tweet.id,
                        'text': tweet.text,
                        'created_at': tweet.created_at.isoformat() if tweet.created_at else None,
                        'author_id': tweet.author_id,
                        'username': getattr(author, 'username', 'unknown') if author else 'unknown',
                        'verified': getattr(author, 'verified', False) if author else False,
                        'retweet_count': metrics.get('retweet_count', 0),
                        'like_count': metrics.get('like_count', 0),
                        'reply_count': metrics.get('reply_count', 0),
                        'quote_count': metrics.get('quote_count', 0),
                        'engagement_score': engagement_score,
                        'url': f"https://twitter.com/{getattr(author, 'username', 'unknown') if author else 'unknown'}/status/{tweet.id}"
                    }
                    tweets_data.append(tweet_data)
                
                # æŒ‰äº’å‹•åº¦æ’åºï¼Œå–æœ€å¥½çš„
                tweets_data.sort(key=lambda x: x['engagement_score'], reverse=True)
                tweets_data = tweets_data[:target_tweets]
                
                self.logger.info(f"   âœ… {category}: æˆåŠŸç²å¾— {len(tweets_data)} æ¢æ¨æ–‡")
                return tweets_data
                
            except tweepy.TooManyRequests:
                self.logger.warning(f"   âš ï¸ {category}: APIé™åˆ¶ (å˜—è©¦ {attempt + 1})")
                if attempt < max_retries - 1:
                    # æŒ‡æ•¸é€€é¿ï¼š30ç§’ã€2åˆ†é˜ã€5åˆ†é˜
                    wait_time = [30, 120, 300][attempt]
                    self.logger.info(f"   â° ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                    time.sleep(wait_time)
                else:
                    self.logger.error(f"   âŒ {category}: é”åˆ°é‡è©¦ä¸Šé™ï¼Œè·³éæ­¤è³½é“")
                    return []
                    
            except Exception as e:
                self.logger.error(f"   âŒ {category}: éŒ¯èª¤ - {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(10)
                else:
                    return []
        
        return tweets_data

    def crawl_all_categories_distributed(self) -> Dict[str, List[Dict[str, Any]]]:
        """åˆ†æ™‚æ®µçˆ¬å–æ‰€æœ‰è³½é“ - æ¯æ—¥å…¨è¦†è“‹"""
        
        self.logger.info("ğŸš€ é–‹å§‹æ¯æ—¥å…¨è¦†è“‹Web3çˆ¬å–...")
        self.logger.info("ğŸ¯ ç›®æ¨™: æ¶µè“‹æ‰€æœ‰7å€‹Web3è³½é“")
        
        all_tweets = {}
        total_crawled = 0
        successful_categories = 0
        
        categories_list = list(self.web3_categories.items())
        random.shuffle(categories_list)  # éš¨æ©Ÿé †åºé¿å…æ¨¡å¼
        
        for i, (category, keyword) in enumerate(categories_list):
            self.logger.info(f"ğŸ“Š è™•ç† {category} ({i+1}/{len(categories_list)})...")
            
            # çˆ¬å–é€™å€‹è³½é“
            tweets = self.crawl_single_category_safe(category, keyword, target_tweets=15)
            all_tweets[category] = tweets
            
            if tweets:
                total_crawled += len(tweets)
                successful_categories += 1
                self.logger.info(f"âœ… {category}: {len(tweets)} æ¢æ¨æ–‡ï¼Œç´¯è¨ˆ {total_crawled}")
            else:
                self.logger.warning(f"âš ï¸ {category}: æœªç²å¾—æ¨æ–‡")
            
            # é¡åˆ¥é–“æ™ºèƒ½å»¶é²
            if i < len(categories_list) - 1:  # ä¸æ˜¯æœ€å¾Œä¸€å€‹
                # åŸºæ–¼æˆåŠŸç‡èª¿æ•´å»¶é²
                if tweets:  # æˆåŠŸäº†
                    delay = random.uniform(90, 150)  # 1.5-2.5åˆ†é˜
                else:  # å¤±æ•—äº†
                    delay = random.uniform(180, 300)  # 3-5åˆ†é˜
                
                self.logger.info(f"â° ç­‰å¾… {delay:.0f} ç§’å¾Œè™•ç†ä¸‹ä¸€è³½é“...")
                time.sleep(delay)
        
        # çµæœçµ±è¨ˆ
        self.logger.info("ğŸ‰ æ¯æ—¥å…¨è¦†è“‹çˆ¬å–å®Œæˆï¼")
        self.logger.info(f"ğŸ“ˆ æˆåŠŸè³½é“: {successful_categories}/{len(categories_list)}")
        self.logger.info(f"ğŸ“Š ç¸½æ¨æ–‡æ•¸: {total_crawled}")
        
        # é¡¯ç¤ºå„è³½é“çµæœ
        for category, tweets in all_tweets.items():
            if tweets:
                avg_engagement = sum(t['engagement_score'] for t in tweets) / len(tweets)
                self.logger.info(f"   âœ… {category}: {len(tweets)}æ¢ï¼Œå¹³å‡ç†±åº¦ {avg_engagement:.1f}")
            else:
                self.logger.info(f"   âŒ {category}: 0æ¢")
        
        return all_tweets

    def save_results(self, data: Dict[str, List[Dict[str, Any]]]):
        """ä¿å­˜çµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ä¿å­˜
        json_filename = f"full_coverage_web3_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # CSV ä¿å­˜ (æ‰€æœ‰æ¨æ–‡åˆä½µ)
        all_tweets = []
        for category, tweets in data.items():
            all_tweets.extend(tweets)
        
        if all_tweets:
            # æŒ‰ç†±åº¦æ’åº
            all_tweets.sort(key=lambda x: x.get('engagement_score', 0), reverse=True)
            
            csv_filename = f"full_coverage_web3_{timestamp}.csv"
            with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=all_tweets[0].keys())
                writer.writeheader()
                writer.writerows(all_tweets)
            
            self.logger.info(f"ğŸ’¾ çµæœå·²ä¿å­˜: {json_filename} & {csv_filename}")
        
        return json_filename

def main():
    BEARER_TOKEN = "AAAAAAAAAAAAAAAAAAAAAF833wEAAAAAVK2bhuSiu%2FaikoUWzmEQvdS%2BJhE%3DjNPAILRXsZOyy1waEYDjahABCRLjG8d9LLyLMAF0CQ3LCckCPq"
    
    print("ğŸŒ æ¯æ—¥å…¨è¦†è“‹Web3çˆ¬èŸ²")
    print("=" * 50)
    print("ğŸ¯ ç­–ç•¥: æ¯æ—¥æ¶µè“‹æ‰€æœ‰7å€‹Web3è³½é“")
    print("âš¡ æ–¹æ³•: åˆ†æ™‚æ®µ + æ™ºèƒ½é‡è©¦ + æŒ‡æ•¸é€€é¿")
    print("ğŸ“Š ç›®æ¨™: æ¯è³½é“15æ¢ç²¾é¸æ¨æ–‡")
    print("=" * 50)
    
    crawler = FullCoverageWeb3Crawler(BEARER_TOKEN)
    
    # åŸ·è¡Œçˆ¬å–
    results = crawler.crawl_all_categories_distributed()
    
    # ä¿å­˜çµæœ
    filename = crawler.save_results(results)
    
    # é¡¯ç¤ºæœ€çµ‚çµ±è¨ˆ
    print("\nğŸ“Š æ¯æ—¥å…¨è¦†è“‹çµæœæ‘˜è¦:")
    successful_categories = 0
    total_tweets = 0
    
    for category, tweets in results.items():
        if tweets:
            successful_categories += 1
            total_tweets += len(tweets)
            avg_engagement = sum(t.get('engagement_score', 0) for t in tweets) / len(tweets)
            print(f"   âœ… {category}: {len(tweets)} æ¢æ¨æ–‡ (ç†±åº¦: {avg_engagement:.1f})")
        else:
            print(f"   âŒ {category}: ç„¡æ¨æ–‡")
    
    coverage_rate = (successful_categories / 7) * 100
    print(f"\nğŸ‰ è¦†è“‹ç‡: {coverage_rate:.1f}% ({successful_categories}/7 è³½é“)")
    print(f"ğŸ“ˆ ç¸½æ¨æ–‡: {total_tweets} æ¢")
    
    if coverage_rate >= 80:
        print("âœ… å„ªç§€ï¼å¹¾ä¹å®Œæ•´è¦†è“‹æ‰€æœ‰Web3è³½é“")
    elif coverage_rate >= 60:
        print("âœ… è‰¯å¥½ï¼è¦†è“‹å¤§éƒ¨åˆ†Web3è³½é“")
    else:
        print("âš ï¸ éƒ¨åˆ†æˆåŠŸï¼Œå¯èƒ½éœ€è¦èª¿æ•´APIä½¿ç”¨ç­–ç•¥")

if __name__ == "__main__":
    main()