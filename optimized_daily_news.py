#!/usr/bin/env python3
"""
å„ªåŒ–ç‰ˆæ¯æ—¥Web3æ–°è - åŸºæ–¼å·²é©—è­‰çš„çˆ¬èŸ²ï¼ŒåŠ å…¥æ™ºèƒ½ç­–ç•¥
"""

import sys
import os
import logging
import time
from datetime import datetime
from twitter_web3_crawler import TwitterWeb3Crawler
from news_reporter import Web3NewsReporter
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

def setup_logging():
    """è¨­ç½®æ—¥èªŒ"""
    timestamp = datetime.now().strftime("%Y%m%d")
    log_filename = f"optimized_news_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

def run_optimized_pipeline():
    """åŸ·è¡Œå„ªåŒ–ç‰ˆæ¯æ—¥æ–°èæµç¨‹"""
    
    logger = setup_logging()
    logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œå„ªåŒ–ç‰ˆWeb3æ–°èæµç¨‹...")
    
    # Twitter APIè¨­å®š
    TWITTER_BEARER_TOKEN = "AAAAAAAAAAAAAAAAAAAAAF833wEAAAAAVK2bhuSiu%2FaikoUWzmEQvdS%2BJhE%3DjNPAILRXsZOyy1waEYDjahABCRLjG8d9LLyLMAF0CQ3LCckCPq"
    
    # OpenAIå’ŒLINEè¨­å®š
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'your_openai_api_key_here')
    LINE_ACCESS_TOKEN = os.getenv('LINE_CHANNEL_ACCESS_TOKEN', 'your_line_channel_access_token_here')
    LINE_USER_ID = os.getenv('LINE_USER_ID', 'your_line_user_id_here')
    
    success_count = 0
    total_steps = 3
    
    try:
        # ===== æ­¥é©Ÿ1ï¼šå„ªåŒ–ç‰ˆTwitterçˆ¬å– =====
        logger.info("ğŸ“Š æ­¥é©Ÿ1/3ï¼šå„ªåŒ–ç‰ˆTwitterç²¾é¸çˆ¬å–...")
        
        crawler = TwitterWeb3Crawler(TWITTER_BEARER_TOKEN)
        
        # å„ªå…ˆç´šé¡åˆ¥é…ç½®ï¼ˆåŸºæ–¼é‡è¦æ€§ï¼‰
        priority_categories = [
            ("DeFi", 30),           # æœ€é‡è¦ï¼Œå¤šæŠ“ä¸€äº›
            ("Layer1_Layer2", 25),  # é‡è¦
            ("NFT_GameFi", 20),     # ä¸­ç­‰é‡è¦
            ("AI_Crypto", 15),      # èˆˆè¶£é¡åˆ¥
            ("Infrastructure", 15), # åŸºç¤è¨­æ–½
            ("RWA", 10),           # æ–°èˆˆé ˜åŸŸ
            ("Meme_Coins", 5)      # å¨›æ¨‚é¡åˆ¥ï¼Œæœ€å°‘
        ]
        
        all_tweets = {}
        total_crawled = 0
        max_daily_tweets = 140  # æ¯æ—¥ç²¾é¸é™åˆ¶
        
        for category, target_count in priority_categories:
            if total_crawled >= max_daily_tweets:
                logger.info(f"âš ï¸ é”åˆ°æ¯æ—¥ç²¾é¸é™åˆ¶ ({max_daily_tweets} æ¢)ï¼Œåœæ­¢çˆ¬å–")
                break
            
            logger.info(f"ğŸ¯ çˆ¬å– {category} é¡åˆ¥ï¼Œç›®æ¨™ {target_count} æ¢ç²¾é¸æ¨æ–‡")
            
            # ä½¿ç”¨åŸå§‹çˆ¬èŸ²æ–¹æ³•ï¼Œä½†èª¿æ•´åƒæ•¸
            try:
                category_tweets = crawler.search_tweets_by_category(
                    category, 
                    crawler.web3_categories[category],
                    max_results=target_count
                )
                
                if category_tweets:
                    # æŒ‰äº’å‹•åº¦æ’åºï¼Œå–æœ€å¥½çš„
                    category_tweets.sort(
                        key=lambda x: x.get('like_count', 0) + x.get('retweet_count', 0) * 2, 
                        reverse=True
                    )
                    
                    # é™åˆ¶åœ¨ç›®æ¨™æ•¸é‡å…§
                    category_tweets = category_tweets[:target_count]
                    all_tweets[category] = category_tweets
                    total_crawled += len(category_tweets)
                    
                    logger.info(f"âœ… {category}: æˆåŠŸç²å¾— {len(category_tweets)} æ¢ï¼Œç´¯è¨ˆ {total_crawled}")
                else:
                    logger.warning(f"âš ï¸ {category}: æœªç²å¾—ä»»ä½•æ¨æ–‡")
                    all_tweets[category] = []
                
            except Exception as e:
                logger.warning(f"âš ï¸ {category} çˆ¬å–å¤±æ•—: {str(e)}")
                all_tweets[category] = []
            
            # æ™ºèƒ½å»¶é²ï¼šé¿å…APIé™åˆ¶
            if category != priority_categories[-1][0]:  # ä¸æ˜¯æœ€å¾Œä¸€å€‹é¡åˆ¥
                logger.info("â° ç­‰å¾…10ç§’é¿å…APIé™åˆ¶...")
                time.sleep(10)
        
        if total_crawled == 0:
            # å¦‚æœå®Œå…¨å¤±æ•—ï¼Œå˜—è©¦åŠ è¼‰ä¹‹å‰çš„æ•¸æ“š
            logger.warning("âš ï¸ æœ¬æ¬¡çˆ¬å–å¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ä¹‹å‰çš„æ•¸æ“š...")
            import glob
            import os
            import json
            
            json_files = glob.glob("*web3_tweets*.json")
            if json_files:
                latest_file = max(json_files, key=os.path.getctime)
                logger.info(f"ä½¿ç”¨æ•¸æ“šæ–‡ä»¶: {latest_file}")
                with open(latest_file, 'r', encoding='utf-8') as f:
                    all_tweets = json.load(f)
                total_crawled = sum(len(tweets) for tweets in all_tweets.values())
            else:
                logger.error("âŒ æ²’æœ‰ä»»ä½•å¯ç”¨æ•¸æ“š")
                return False
        
        # ä¿å­˜æ•¸æ“š
        crawler.save_to_json(all_tweets)
        crawler.save_to_csv(all_tweets)
        
        logger.info(f"âœ… æ­¥é©Ÿ1å®Œæˆï¼šæˆåŠŸç²å¾— {total_crawled} æ¢ç²¾é¸æ¨æ–‡")
        success_count += 1
        
        # ===== æ­¥é©Ÿ2ï¼šAIæ–°èåˆ†æ =====
        logger.info("ğŸ¤– æ­¥é©Ÿ2/3ï¼šAIæ™ºèƒ½æ–°èåˆ†æ...")
        
        reporter = Web3NewsReporter(
            openai_api_key=OPENAI_API_KEY,
            line_access_token=LINE_ACCESS_TOKEN, 
            line_user_id=LINE_USER_ID
        )
        
        # ç”Ÿæˆå„ªåŒ–ç‰ˆæç¤ºï¼Œå¼·èª¿ç²¾é¸å…§å®¹
        report = reporter.analyze_tweets_with_openai(all_tweets)
        
        if not report or "éŒ¯èª¤" in report:
            logger.error("âŒ AIæ–°èåˆ†æå¤±æ•—")
            return False
        
        # ä¿å­˜å ±å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"optimized_news_{timestamp}.txt"
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(f"ğŸ“Š åŸºæ–¼ {total_crawled} æ¢ç²¾é¸æ¨æ–‡ç”Ÿæˆ\n")
            f.write("="*50 + "\n\n")
            f.write(report)
        
        logger.info(f"âœ… æ­¥é©Ÿ2å®Œæˆï¼šå ±å‘Šå·²ä¿å­˜åˆ° {report_filename}")
        success_count += 1
        
        # ===== æ­¥é©Ÿ3ï¼šLINEæ¨æ’­ =====
        logger.info("ğŸ“± æ­¥é©Ÿ3/3ï¼šç™¼é€ç²¾é¸æ–°èåˆ°LINE...")
        
        # åœ¨å ±å‘Šå‰åŠ å…¥çµ±è¨ˆä¿¡æ¯
        enhanced_report = f"ğŸ“Š ä»Šæ—¥Web3ç²¾é¸ ({total_crawled}æ¢æ¨æ–‡)\n" + "="*30 + "\n\n" + report
        
        line_success = reporter.send_to_line(enhanced_report)
        
        if line_success:
            logger.info("âœ… æ­¥é©Ÿ3å®Œæˆï¼šç²¾é¸æ–°èå·²æ¨é€åˆ°LINE")
            success_count += 1
        else:
            logger.error("âŒ LINEæ¨é€å¤±æ•—")
        
        # ===== ç¸½çµ =====
        logger.info("=" * 50)
        logger.info(f"ğŸ“‹ å„ªåŒ–æµç¨‹å®Œæˆæ‘˜è¦ï¼š")
        logger.info(f"   æˆåŠŸæ­¥é©Ÿ: {success_count}/{total_steps}")
        logger.info(f"   ç²¾é¸æ¨æ–‡: {total_crawled} æ¢")
        logger.info(f"   å ±å‘Šæ–‡ä»¶: {report_filename}")
        logger.info(f"   LINEæ¨æ’­: {'âœ… æˆåŠŸ' if line_success else 'âŒ å¤±æ•—'}")
        
        if success_count == total_steps:
            logger.info("ğŸ‰ å„ªåŒ–ç‰ˆWeb3æ–°èæµç¨‹åŸ·è¡ŒæˆåŠŸï¼")
            return True
        else:
            logger.warning("âš ï¸ æµç¨‹éƒ¨åˆ†æˆåŠŸï¼Œè«‹æª¢æŸ¥æ—¥èªŒ")
            return False
            
    except Exception as e:
        logger.error(f"âŒ æµç¨‹åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ“° å„ªåŒ–ç‰ˆWeb3æ–°èç³»çµ±")
    print("=" * 50)
    print("ğŸ¯ ç‰¹è‰²ï¼š140æ¢æ¯æ—¥ç²¾é¸ + å„ªå…ˆç´šçˆ¬å– + æ™ºèƒ½åˆ†æ")
    print("=" * 50)
    
    # åŸ·è¡Œå„ªåŒ–æµç¨‹
    success = run_optimized_pipeline()
    
    if success:
        print("\nğŸŠ æ­å–œï¼å„ªåŒ–ç‰ˆæ–°èæµç¨‹åŸ·è¡ŒæˆåŠŸ")
        print("ğŸ“± è«‹æª¢æŸ¥LINEæ¥æ”¶ç²¾é¸Web3æ–°èå ±å‘Š")
        print("ğŸ’¡ å ±å‘ŠåŸºæ–¼é«˜äº’å‹•åº¦ç²¾é¸æ¨æ–‡ç”Ÿæˆï¼Œè³ªé‡æ›´é«˜")
    else:
        print("\nâŒ æµç¨‹åŸ·è¡Œå¤±æ•—")
        print("ğŸ’¡ å»ºè­°æª¢æŸ¥æ—¥èªŒæ–‡ä»¶äº†è§£è©³æƒ…")

if __name__ == "__main__":
    main()