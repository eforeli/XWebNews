#!/usr/bin/env python3
"""
æ™ºèƒ½Web3 Twitterçˆ¬èŸ² - å„ªåŒ–ç‰ˆæœ¬
é‡å°APIé™åˆ¶é€²è¡Œå„ªåŒ–ï¼Œæ”¯æŒåˆ†æ‰¹åŸ·è¡Œå’Œæ™ºèƒ½é‡è©¦
"""

import tweepy
import json
import time
import csv
from datetime import datetime, timedelta
from typing import List, Dict, Any
import logging
import random

class SmartWeb3Crawler:
    def __init__(self, bearer_token: str):
        """
        åˆå§‹åŒ–æ™ºèƒ½Web3 Twitterçˆ¬èŸ²
        
        Args:
            bearer_token: Twitter API Bearer Token
        """
        self.client = tweepy.Client(bearer_token=bearer_token)
        self.setup_logging()
        
        # æ¯æ—¥åˆ†é…ç­–ç•¥ï¼š7å€‹é¡åˆ¥ Ã— æ¯é¡åˆ¥20æ¢ = 140æ¢æ¨æ–‡/å¤©
        # ç¢ºä¿åœ¨10,000æ¢/æœˆé™åˆ¶å…§ (140 Ã— 30 = 4,200æ¢/æœˆ)
        self.daily_tweet_limit = 140
        self.tweets_per_category = 20
        
        # Web3è³½é“é—œéµå­— - å„ªåŒ–ç‰ˆï¼ˆæ›´ç²¾æº–ï¼‰
        self.web3_categories = {
            "DeFi": {
                "keywords": ["DeFi", "UniSwap", "SushiSwap", "Compound", "Aave", "Curve", "PancakeSwap"],
                "priority": 1  # æœ€é«˜å„ªå…ˆç´š
            },
            "Layer1_Layer2": {
                "keywords": ["Ethereum", "Solana", "Polygon", "Arbitrum", "Optimism", "Base", "zkSync"],
                "priority": 2
            },
            "NFT_GameFi": {
                "keywords": ["NFT", "OpenSea", "GameFi", "play to earn", "P2E", "Axie", "metaverse"],
                "priority": 3
            },
            "AI_Crypto": {
                "keywords": ["AI crypto", "ChatGPT", "artificial intelligence", "FET", "AGIX", "OCEAN"],
                "priority": 2
            },
            "RWA": {
                "keywords": ["RWA", "real world assets", "tokenization", "BlackRock", "asset backed"],
                "priority": 3
            },
            "Meme_Coins": {
                "keywords": ["DOGE", "SHIB", "PEPE", "meme coin", "Dogecoin", "Shiba Inu"],
                "priority": 4  # æœ€ä½å„ªå…ˆç´š
            },
            "Infrastructure": {
                "keywords": ["Chainlink", "oracle", "cross-chain", "bridge", "LINK", "interoperability"],
                "priority": 2
            }
        }

    def setup_logging(self):
        """è¨­ç½®æ—¥èªŒ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('smart_crawler.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def get_rate_limit_status(self):
        """æª¢æŸ¥APIä½¿ç”¨ç‹€æ³"""
        try:
            # æ³¨æ„ï¼šé€™å€‹èª¿ç”¨æœ¬èº«ä¹Ÿæœƒæ¶ˆè€—APIé…é¡
            limits = self.client.get_rate_limit_status()
            return limits
        except Exception as e:
            self.logger.warning(f"ç„¡æ³•ç²å–é€Ÿç‡é™åˆ¶ç‹€æ³: {str(e)}")
            return None

    def smart_search_tweets(self, category: str, config: Dict, max_results: int = 20) -> List[Dict[str, Any]]:
        """
        æ™ºèƒ½æœå°‹æ¨æ–‡ - åŠ å…¥é‡è©¦é‚è¼¯å’ŒéŒ¯èª¤è™•ç†
        
        Args:
            category: Web3é¡åˆ¥åç¨±
            config: é¡åˆ¥é…ç½®ï¼ˆé—œéµå­—å’Œå„ªå…ˆç´šï¼‰
            max_results: æœ€å¤§çµæœæ•¸é‡
            
        Returns:
            æ¨æ–‡æ•¸æ“šåˆ—è¡¨
        """
        tweets_data = []
        keywords = config["keywords"]
        
        # æ§‹å»ºæ›´ç²¾æº–çš„æŸ¥è©¢å­—ç¬¦ä¸²
        # ä½¿ç”¨é«˜åƒ¹å€¼é—œéµå­—ï¼Œéæ¿¾ä½è³ªé‡å…§å®¹
        primary_keywords = keywords[:3]  # ä½¿ç”¨å‰3å€‹æœ€é‡è¦çš„é—œéµå­—
        query = " OR ".join([f'"{keyword}"' for keyword in primary_keywords])
        
        # æ·»åŠ éæ¿¾æ¢ä»¶
        query += " -is:retweet -is:reply lang:en"  # æ’é™¤è½‰æ¨å’Œå›å¾©ï¼Œåªè¦è‹±æ–‡
        query += " has:links"  # åªè¦åŒ…å«é€£çµçš„æ¨æ–‡ï¼ˆé€šå¸¸è³ªé‡è¼ƒé«˜ï¼‰
        
        max_retries = 3
        base_delay = 2
        
        for attempt in range(max_retries):
            try:
                self.logger.info(f"æ­£åœ¨æœå°‹ {category} é¡åˆ¥... (å˜—è©¦ {attempt + 1}/{max_retries})")
                self.logger.debug(f"æŸ¥è©¢å­—ç¬¦ä¸²: {query}")
                
                # ç›´æ¥èª¿ç”¨search_recent_tweetsï¼Œé¿å…Paginatorçš„è¤‡é›œæ€§
                response = self.client.search_recent_tweets(
                    query=query,
                    tweet_fields=['created_at', 'author_id', 'public_metrics'],
                    user_fields=['username', 'verified'],
                    expansions=['author_id'],
                    max_results=min(max_results, 100)
                )
                
                tweets = response.data if response.data else []
                
                users = {}
                # è™•ç†ç”¨æˆ¶æ•¸æ“š
                if hasattr(response, 'includes') and response.includes and 'users' in response.includes:
                    users = {user.id: user for user in response.includes['users']}
                
                for tweet in tweets:
                    # ç²å–ç”¨æˆ¶ä¿¡æ¯
                    author = users.get(tweet.author_id)
                    
                    # è¨ˆç®—è³ªé‡åˆ†æ•¸ï¼ˆç”¨æ–¼æ’åºï¼‰
                    quality_score = (
                        tweet.public_metrics['like_count'] * 2 +
                        tweet.public_metrics['retweet_count'] * 3 +
                        tweet.public_metrics['reply_count'] * 1
                    )
                    
                    tweet_data = {
                        'category': category,
                        'tweet_id': tweet.id,
                        'text': tweet.text,
                        'created_at': tweet.created_at.isoformat() if tweet.created_at else None,
                        'author_id': tweet.author_id,
                        'username': getattr(author, 'username', 'unknown') if author else 'unknown',
                        'verified': getattr(author, 'verified', False) if author else False,
                        'retweet_count': tweet.public_metrics['retweet_count'] if tweet.public_metrics else 0,
                        'like_count': tweet.public_metrics['like_count'] if tweet.public_metrics else 0,
                        'reply_count': tweet.public_metrics['reply_count'] if tweet.public_metrics else 0,
                        'quote_count': tweet.public_metrics['quote_count'] if tweet.public_metrics else 0,
                        'quality_score': quality_score,
                        'url': f"https://twitter.com/{getattr(author, 'username', 'unknown') if author else 'unknown'}/status/{tweet.id}"
                    }
                    tweets_data.append(tweet_data)
                
                # æŒ‰è³ªé‡åˆ†æ•¸æ’åºï¼Œå–æœ€å¥½çš„
                tweets_data.sort(key=lambda x: x['quality_score'], reverse=True)
                tweets_data = tweets_data[:max_results]
                
                self.logger.info(f"âœ… {category} é¡åˆ¥æ‰¾åˆ° {len(tweets_data)} æ¢é«˜è³ªé‡æ¨æ–‡")
                return tweets_data
                
            except tweepy.TooManyRequests:
                wait_time = base_delay * (2 ** attempt) + random.uniform(0, 1)
                self.logger.warning(f"âŒ APIé™åˆ¶ - {category}ï¼Œç­‰å¾… {wait_time:.1f} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)
                
            except Exception as e:
                self.logger.error(f"æœå°‹ {category} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                if attempt == max_retries - 1:
                    break
                time.sleep(base_delay)
        
        return tweets_data

    def crawl_by_priority(self) -> Dict[str, List[Dict[str, Any]]]:
        """
        æŒ‰å„ªå…ˆç´šçˆ¬å–æ•¸æ“š - ç¢ºä¿é‡è¦é¡åˆ¥å„ªå…ˆç²å¾—æ•¸æ“š
        
        Returns:
            æŒ‰é¡åˆ¥åˆ†çµ„çš„æ¨æ–‡æ•¸æ“š
        """
        self.logger.info("ğŸ¯ é–‹å§‹æ™ºèƒ½å„ªå…ˆç´šçˆ¬å–...")
        
        # æŒ‰å„ªå…ˆç´šæ’åºé¡åˆ¥
        sorted_categories = sorted(
            self.web3_categories.items(),
            key=lambda x: x[1]["priority"]
        )
        
        all_tweets = {}
        total_crawled = 0
        
        for category, config in sorted_categories:
            if total_crawled >= self.daily_tweet_limit:
                self.logger.info(f"âš ï¸  é”åˆ°æ¯æ—¥é™åˆ¶ ({self.daily_tweet_limit} æ¢)ï¼Œåœæ­¢çˆ¬å–")
                break
            
            remaining_quota = self.daily_tweet_limit - total_crawled
            tweets_for_this_category = min(self.tweets_per_category, remaining_quota)
            
            self.logger.info(f"ğŸ“Š çˆ¬å– {category} (å„ªå…ˆç´š {config['priority']})ï¼Œç›®æ¨™ {tweets_for_this_category} æ¢")
            
            tweets = self.smart_search_tweets(category, config, tweets_for_this_category)
            all_tweets[category] = tweets
            
            if tweets:
                total_crawled += len(tweets)
                self.logger.info(f"âœ… æˆåŠŸç²å¾— {len(tweets)} æ¢ï¼Œç¸½è¨ˆ {total_crawled}/{self.daily_tweet_limit}")
            
            # æ™ºèƒ½å»¶é²ï¼šæ ¹æ“šå‰©é¤˜é¡åˆ¥èª¿æ•´ç­‰å¾…æ™‚é–“
            remaining_categories = len(sorted_categories) - len(all_tweets)
            if remaining_categories > 0:
                delay_time = max(3, 15 / remaining_categories)  # å¹³å‡åˆ†é…ç­‰å¾…æ™‚é–“
                self.logger.info(f"â° ç­‰å¾… {delay_time:.1f} ç§’å¾Œç¹¼çºŒ...")
                time.sleep(delay_time)
        
        self.logger.info(f"ğŸ‰ æ™ºèƒ½çˆ¬å–å®Œæˆï¼ç¸½å…±ç²å¾— {total_crawled} æ¢æ¨æ–‡")
        return all_tweets

    def save_to_json(self, data: Dict[str, Any], filename: str = None):
        """ä¿å­˜æ•¸æ“šç‚ºJSONæ–‡ä»¶"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"smart_web3_tweets_{timestamp}.json"
            
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            self.logger.info(f"ğŸ’¾ æ•¸æ“šå·²ä¿å­˜åˆ° {filename}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜JSONæ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

    def save_to_csv(self, data: Dict[str, List[Dict[str, Any]]], filename: str = None):
        """ä¿å­˜æ•¸æ“šç‚ºCSVæ–‡ä»¶"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"smart_web3_tweets_{timestamp}.csv"
            
        try:
            # å±•å¹³æ‰€æœ‰æ¨æ–‡æ•¸æ“š
            all_tweets = []
            for category, tweets in data.items():
                all_tweets.extend(tweets)
                
            if all_tweets:
                # æŒ‰è³ªé‡åˆ†æ•¸æ’åº
                all_tweets.sort(key=lambda x: x.get('quality_score', 0), reverse=True)
                
                fieldnames = all_tweets[0].keys()
                with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(all_tweets)
                    
                self.logger.info(f"ğŸ’¾ æ•¸æ“šå·²ä¿å­˜åˆ° {filename}")
            else:
                self.logger.warning("æ²’æœ‰æ•¸æ“šå¯ä¿å­˜")
                
        except Exception as e:
            self.logger.error(f"ä¿å­˜CSVæ–‡ä»¶æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

def main():
    # Twitter API Bearer Token
    BEARER_TOKEN = "AAAAAAAAAAAAAAAAAAAAAF833wEAAAAAVK2bhuSiu%2FaikoUWzmEQvdS%2BJhE%3DjNPAILRXsZOyy1waEYDjahABCRLjG8d9LLyLMAF0CQ3LCckCPq"
    
    print("ğŸš€ æ™ºèƒ½Web3 Twitterçˆ¬èŸ² v2.0")
    print("="*50)
    print("ğŸ¯ ç‰¹è‰²ï¼šå„ªå…ˆç´šçˆ¬å– + æ™ºèƒ½é‡è©¦ + è³ªé‡éæ¿¾")
    print("="*50)
    
    # å‰µå»ºæ™ºèƒ½çˆ¬èŸ²å¯¦ä¾‹
    crawler = SmartWeb3Crawler(BEARER_TOKEN)
    
    # åŸ·è¡Œæ™ºèƒ½çˆ¬å–
    all_tweets = crawler.crawl_by_priority()
    
    # ä¿å­˜æ•¸æ“š
    crawler.save_to_json(all_tweets)
    crawler.save_to_csv(all_tweets)
    
    # é¡¯ç¤ºçµ±è¨ˆ
    print("\nğŸ“Š çˆ¬å–çµæœçµ±è¨ˆï¼š")
    total_tweets = 0
    for category, tweets in all_tweets.items():
        if tweets:
            print(f"   {category}: {len(tweets)} æ¢æ¨æ–‡")
            total_tweets += len(tweets)
    
    print(f"\nğŸ‰ ç¸½å…±æˆåŠŸçˆ¬å– {total_tweets} æ¢é«˜è³ªé‡Web3æ¨æ–‡")
    print("ğŸ’¡ æç¤ºï¼šæ•¸æ“šå·²æŒ‰è³ªé‡åˆ†æ•¸æ’åºï¼Œå„ªå…ˆé¡¯ç¤ºé«˜äº’å‹•åº¦å…§å®¹")

if __name__ == "__main__":
    main()