#!/usr/bin/env python3
"""
æ™ºèƒ½æ¢ä»¶æ¸¬è©¦ - çˆ¬èŸ²æˆåŠŸæ‰åŸ·è¡Œå®Œæ•´ç®¡é“
"""

import time
import subprocess
import json
import glob
import os
from datetime import datetime

def check_api_ready():
    """æª¢æŸ¥APIæ˜¯å¦å¯ç”¨"""
    try:
        result = subprocess.run(['python3', 'check_api_limits.py'], 
                              capture_output=True, text=True)
        return "API æ­£å¸¸é‹ä½œ" in result.stdout
    except:
        return False

def wait_for_api_reset():
    """ç­‰å¾…APIé‡ç½®"""
    print("â° æ­¥é©Ÿ0: ç­‰å¾…APIé™åˆ¶é‡ç½®...")
    print("=" * 50)
    
    start_time = datetime.now()
    max_wait_time = 20 * 60  # æœ€å¤šç­‰å¾…20åˆ†é˜
    check_interval = 30  # æ¯30ç§’æª¢æŸ¥ä¸€æ¬¡
    
    print(f"ğŸ•’ é–‹å§‹æ™‚é–“: {start_time.strftime('%H:%M:%S')}")
    
    elapsed = 0
    while elapsed < max_wait_time:
        print(f"ğŸ” æª¢æŸ¥API... (å·²ç­‰å¾… {elapsed//60}åˆ†{elapsed%60}ç§’)")
        
        if check_api_ready():
            print(f"âœ… APIå·²é‡ç½®ï¼ç­‰å¾…æ™‚é–“: {elapsed//60}åˆ†{elapsed%60}ç§’")
            return True
        
        time.sleep(check_interval)
        elapsed += check_interval
    
    print(f"âŒ ç­‰å¾…è¶…æ™‚ï¼ŒAPIå¯èƒ½éœ€è¦æ›´é•·æ™‚é–“")
    return False

def analyze_crawler_results():
    """åˆ†æçˆ¬èŸ²çµæœï¼Œåˆ¤æ–·æ˜¯å¦æˆåŠŸ"""
    
    # å°‹æ‰¾æœ€æ–°çš„çˆ¬èŸ²çµæœ
    json_files = glob.glob("full_coverage_web3_*.json")
    if not json_files:
        print("âŒ æœªæ‰¾åˆ°çˆ¬èŸ²çµæœæª”æ¡ˆ")
        return False, "ç„¡çµæœæª”æ¡ˆ"
    
    latest_file = max(json_files, key=os.path.getctime)
    print(f"ğŸ“ åˆ†ææª”æ¡ˆ: {latest_file}")
    
    try:
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # çµ±è¨ˆæˆåŠŸçš„è³½é“
        successful_categories = []
        total_tweets = 0
        
        for category, tweets in data.items():
            if tweets and len(tweets) > 0:
                successful_categories.append(category)
                total_tweets += len(tweets)
        
        coverage_rate = (len(successful_categories) / 7) * 100
        
        print(f"ğŸ“Š çˆ¬èŸ²çµæœåˆ†æ:")
        print(f"   âœ… æˆåŠŸè³½é“: {len(successful_categories)}/7 ({coverage_rate:.1f}%)")
        print(f"   ğŸ“ˆ ç¸½æ¨æ–‡æ•¸: {total_tweets}")
        print(f"   ğŸ¯ æˆåŠŸè³½é“: {', '.join(successful_categories)}")
        
        # åˆ¤æ–·æ¨™æº–
        if len(successful_categories) >= 5 and total_tweets >= 50:
            print("ğŸ‰ å„ªç§€çµæœï¼çˆ¬èŸ²å•é¡Œå·²è§£æ±º")
            return True, "å„ªç§€"
        elif len(successful_categories) >= 3 and total_tweets >= 30:
            print("âœ… è‰¯å¥½çµæœï¼æ˜é¡¯æ”¹å–„")
            return True, "è‰¯å¥½"
        elif len(successful_categories) >= 2:
            print("âš ï¸ éƒ¨åˆ†æ”¹å–„ï¼Œä½†ä»éœ€å„ªåŒ–")
            return False, "éœ€å„ªåŒ–"
        else:
            print("âŒ çµæœä¸ç†æƒ³ï¼Œéœ€è¦é‡æ–°æª¢è¨ç­–ç•¥")
            return False, "å¤±æ•—"
            
    except Exception as e:
        print(f"âŒ åˆ†æçµæœæª”æ¡ˆéŒ¯èª¤: {str(e)}")
        return False, "åˆ†æéŒ¯èª¤"

def run_crawler_test():
    """åŸ·è¡Œé¸é …A: çˆ¬èŸ²æ¸¬è©¦"""
    
    print("\nğŸ” é¸é …A: å…¨è¦†è“‹çˆ¬èŸ²æ¸¬è©¦")
    print("=" * 50)
    
    try:
        result = subprocess.run(['python3', 'full_coverage_crawler.py'], 
                              capture_output=True, text=True, timeout=1800)
        
        print("ğŸ“Š çˆ¬èŸ²åŸ·è¡Œå®Œæˆ")
        
        # é¡¯ç¤ºé—œéµè¼¸å‡º
        if "è¦†è“‹ç‡:" in result.stdout:
            coverage_lines = [line for line in result.stdout.split('\n') if "è¦†è“‹ç‡:" in line or "ç¸½æ¨æ–‡:" in line]
            for line in coverage_lines:
                print(f"   {line}")
        
        # åˆ†æçµæœ
        success, quality = analyze_crawler_results()
        return success, quality
        
    except subprocess.TimeoutExpired:
        print("âš ï¸ çˆ¬èŸ²åŸ·è¡Œè¶…æ™‚")
        return False, "è¶…æ™‚"
    except Exception as e:
        print(f"âŒ çˆ¬èŸ²åŸ·è¡ŒéŒ¯èª¤: {str(e)}")
        return False, "éŒ¯èª¤"

def run_complete_pipeline():
    """åŸ·è¡Œé¸é …B: å®Œæ•´ç®¡é“"""
    
    print("\nğŸš€ é¸é …B: å®Œæ•´ç®¡é“æ¸¬è©¦ (çˆ¬èŸ²â†’AIâ†’LINE)")
    print("=" * 50)
    print("ğŸ’¡ å› ç‚ºçˆ¬èŸ²æˆåŠŸï¼Œç¾åœ¨åŸ·è¡Œå®Œæ•´æµç¨‹")
    
    try:
        result = subprocess.run(['python3', 'quick_news_test.py'], 
                              capture_output=True, text=True, timeout=300)
        
        if "LINEæ¨é€æˆåŠŸ" in result.stdout:
            print("ğŸ‰ å®Œæ•´ç®¡é“æˆåŠŸï¼")
            print("ğŸ“± è«‹æª¢æŸ¥ä½ çš„LINEæ¥æ”¶Web3æ–°è")
            return True
        elif "æ¸¬è©¦éƒ¨åˆ†æˆåŠŸ" in result.stdout:
            print("âœ… AIåˆ†ææˆåŠŸï¼ŒLINEå¯èƒ½æœ‰å°å•é¡Œ")
            return True
        else:
            print("âš ï¸ å®Œæ•´ç®¡é“éƒ¨åˆ†å¤±æ•—")
            return False
            
    except subprocess.TimeoutExpired:
        print("âš ï¸ å®Œæ•´ç®¡é“è¶…æ™‚")
        return False
    except Exception as e:
        print(f"âŒ å®Œæ•´ç®¡é“éŒ¯èª¤: {str(e)}")
        return False

def main():
    print("ğŸ§  æ™ºèƒ½æ¢ä»¶æ¸¬è©¦ç³»çµ±")
    print("=" * 50)
    print("ğŸ¯ é‚è¼¯: çˆ¬èŸ²æˆåŠŸ â†’ å®Œæ•´ç®¡é“ | çˆ¬èŸ²å•é¡Œ â†’ å„ªåŒ–çˆ¬èŸ²")
    print("=" * 50)
    
    # æ­¥é©Ÿ0: ç­‰å¾…APIé‡ç½®
    if not wait_for_api_reset():
        print("âŒ APIæœªé‡ç½®ï¼Œç„¡æ³•åŸ·è¡Œæ¸¬è©¦")
        return
    
    # æ­¥é©Ÿ1: åŸ·è¡Œçˆ¬èŸ²æ¸¬è©¦
    crawler_success, quality = run_crawler_test()
    
    # æ±ºç­–é‚è¼¯
    if crawler_success:
        print(f"\nâœ… çˆ¬èŸ²æ¸¬è©¦æˆåŠŸ (è³ªé‡: {quality})")
        print("ğŸ”„ ç¹¼çºŒåŸ·è¡Œå®Œæ•´ç®¡é“æ¸¬è©¦...")
        
        # æ­¥é©Ÿ2: åŸ·è¡Œå®Œæ•´ç®¡é“
        pipeline_success = run_complete_pipeline()
        
        # æœ€çµ‚ç¸½çµ
        print("\n" + "=" * 50)
        print("ğŸ“‹ æ™ºèƒ½æ¸¬è©¦å®Œæˆç¸½çµ")
        print("=" * 50)
        print(f"âœ… çˆ¬èŸ²æ¸¬è©¦: æˆåŠŸ ({quality})")
        print(f"{'âœ…' if pipeline_success else 'âš ï¸'} å®Œæ•´ç®¡é“: {'æˆåŠŸ' if pipeline_success else 'éƒ¨åˆ†æˆåŠŸ'}")
        
        if pipeline_success:
            print("\nğŸŠ å®Œç¾ï¼å¤šè³½é“å•é¡Œå·²è§£æ±º + å®Œæ•´æµç¨‹æ­£å¸¸")
            print("ğŸ“… æ˜å¤©8:00è‡ªå‹•æ’ç¨‹æœƒå¾ˆå¯é ")
            print("ğŸ“± ä½ æ‡‰è©²å·²æ”¶åˆ°LINEæ¨é€çš„Web3æ–°è")
        else:
            print("\nâœ… çˆ¬èŸ²å•é¡Œå·²è§£æ±ºï¼ŒLINEæ¨é€éƒ¨åˆ†éœ€è¦æª¢æŸ¥")
            print("ğŸ’¡ æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸ï¼Œå¯ä»¥æ”¾å¿ƒä½¿ç”¨")
            
    else:
        print(f"\nâŒ çˆ¬èŸ²æ¸¬è©¦æœªé”æ¨™ (ç‹€æ³: {quality})")
        print("ğŸ”§ ä¸åŸ·è¡Œå®Œæ•´ç®¡é“ï¼Œå„ªå…ˆè§£æ±ºçˆ¬èŸ²å•é¡Œ")
        
        # åˆ†æå•é¡Œ
        print("\nğŸ’¡ å•é¡Œåˆ†æå»ºè­°:")
        if quality == "éœ€å„ªåŒ–":
            print("   - APIé™åˆ¶ä»å¤ªåš´æ ¼ï¼Œå»ºè­°å¢åŠ å»¶é²æ™‚é–“")
            print("   - è€ƒæ…®é€²ä¸€æ­¥ç°¡åŒ–é—œéµå­—ç­–ç•¥")
        elif quality == "å¤±æ•—":
            print("   - å¯èƒ½éœ€è¦é‡æ–°è¨­è¨ˆçˆ¬èŸ²ç­–ç•¥")
            print("   - æª¢æŸ¥Twitter APIé…é¡æ˜¯å¦æœ‰å…¶ä»–é™åˆ¶")
        else:
            print("   - æª¢æŸ¥åŸ·è¡Œæ—¥èªŒæ‰¾å‡ºå…·é«”å•é¡Œ")
    
    # ä¿å­˜æ¸¬è©¦å ±å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_filename = f"smart_test_report_{timestamp}.txt"
    
    report_content = f"""
æ™ºèƒ½æ¢ä»¶æ¸¬è©¦å ±å‘Š
=====================================
æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
æ¸¬è©¦é‚è¼¯: çˆ¬èŸ²æˆåŠŸâ†’å®Œæ•´ç®¡é“ | çˆ¬èŸ²å•é¡Œâ†’å„ªåŒ–çˆ¬èŸ²

çµæœ:
- çˆ¬èŸ²æ¸¬è©¦: {'æˆåŠŸ' if crawler_success else 'å¤±æ•—'} ({quality})
- å®Œæ•´ç®¡é“: {'åŸ·è¡ŒæˆåŠŸ' if crawler_success and 'pipeline_success' in locals() and pipeline_success else 'æœªåŸ·è¡Œæˆ–å¤±æ•—'}

å¤šè³½é“è¦†è“‹å•é¡Œ: {'âœ… å·²è§£æ±º' if crawler_success else 'âŒ ä»éœ€å„ªåŒ–'}

ä¸‹ä¸€æ­¥:
{'ç³»çµ±æº–å‚™å¥½æ¯æ—¥è‡ªå‹•åŸ·è¡Œ' if crawler_success else 'éœ€è¦é€²ä¸€æ­¥å„ªåŒ–çˆ¬èŸ²ç­–ç•¥'}
=====================================
"""
    
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"\nğŸ“Š æ™ºèƒ½æ¸¬è©¦å ±å‘Š: {report_filename}")

if __name__ == "__main__":
    main()