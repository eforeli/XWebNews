#!/usr/bin/env python3
"""
è¼ªæ›¿å¼Web3çˆ¬èŸ² - è§£æ±ºå¤šè³½é“å•é¡Œçš„æœ€çµ‚æ–¹æ¡ˆ
æ¯æ—¥åªçˆ¬2-3å€‹è³½é“ï¼Œè¼ªæ›¿é€²è¡Œï¼Œç¢ºä¿æ‰€æœ‰è³½é“éƒ½æœ‰è¦†è“‹
"""

import tweepy
import json
import time
import csv
from datetime import datetime, timedelta
from typing import List, Dict, Any
import logging
import os
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# å°å…¥ LINE Bot å ±å‘Šå™¨
try:
    from news_reporter import Web3NewsReporter
    NEWS_REPORTER_AVAILABLE = True
    print("âœ… News reporter module imported successfully")
except ImportError as e:
    NEWS_REPORTER_AVAILABLE = False
    print(f"âŒ Failed to import news reporter: {e}")

class RotationalWeb3Crawler:
    def __init__(self, bearer_token: str):
        """è¼ªæ›¿å¼çˆ¬èŸ² - æ™ºèƒ½é¸æ“‡ä»Šæ—¥è¦çˆ¬çš„è³½é“"""
        self.client = tweepy.Client(bearer_token=bearer_token)
        self.setup_logging()
        
        # 7å€‹è³½é“çš„è¼ªæ›¿è¨ˆåŠƒ
        self.web3_categories = {
            "DeFi": ["DeFi", "Uniswap", "compound"],
            "Layer1_Layer2": ["Ethereum", "Solana", "Polygon"], 
            "NFT_GameFi": ["NFT", "OpenSea", "gaming"],
            "AI_Crypto": ["AI", "ChatGPT", "artificial intelligence"],
            "RWA": ["RWA", "tokenization", "BlackRock"],
            "Meme_Coins": ["DOGE", "SHIB", "PEPE"],
            "Infrastructure": ["Chainlink", "oracle", "bridge"]
        }
        
        # è¼ªæ›¿ç‹€æ…‹æª”æ¡ˆ
        self.rotation_file = "crawler_rotation_state.json"

    def setup_logging(self):
        """è¨­ç½®æ—¥èªŒ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('rotational_crawler.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def get_todays_categories(self) -> List[str]:
        """æ™ºèƒ½é¸æ“‡ä»Šæ—¥è¦çˆ¬çš„è³½é“"""
        
        # è®€å–è¼ªæ›¿ç‹€æ…‹
        if os.path.exists(self.rotation_file):
            try:
                with open(self.rotation_file, 'r') as f:
                    rotation_state = json.load(f)
            except:
                rotation_state = {"last_crawled": {}, "rotation_index": 0}
        else:
            rotation_state = {"last_crawled": {}, "rotation_index": 0}
        
        # æ‰€æœ‰è³½é“
        all_categories = list(self.web3_categories.keys())
        
        # ä»Šæ—¥æ—¥æœŸ
        today = datetime.now().strftime("%Y-%m-%d")
        
        # è¼ªæ›¿é‚è¼¯ï¼šæ¯æ—¥çˆ¬2å€‹è³½é“ï¼Œç¢ºä¿ä¸€é€±å…§è¦†è“‹æ‰€æœ‰7å€‹è³½é“
        start_index = rotation_state.get("rotation_index", 0)
        todays_categories = []
        
        # é¸æ“‡2å€‹è³½é“
        for i in range(2):
            category_index = (start_index + i) % len(all_categories)
            todays_categories.append(all_categories[category_index])
        
        # æ›´æ–°è¼ªæ›¿ç‹€æ…‹
        rotation_state["rotation_index"] = (start_index + 2) % len(all_categories)
        
        # ç¢ºä¿ last_crawled å­˜åœ¨
        if "last_crawled" not in rotation_state:
            rotation_state["last_crawled"] = {}
        
        rotation_state["last_crawled"][today] = todays_categories
        
        # ä¿å­˜ç‹€æ…‹
        with open(self.rotation_file, 'w') as f:
            json.dump(rotation_state, f, indent=2)
        
        self.logger.info(f"ğŸ“… ä»Šæ—¥é¸å®šè³½é“: {', '.join(todays_categories)}")
        return todays_categories

    def crawl_single_category(self, category: str, keywords: List[str], max_results: int = 40) -> List[Dict[str, Any]]:
        """çˆ¬å–å–®ä¸€è³½é“ - ä½¿ç”¨æœ€ç°¡å–®ç­–ç•¥"""
        
        tweets_data = []
        
        # ä½¿ç”¨æœ€ç°¡å–®çš„é—œéµå­—
        primary_keyword = keywords[0]
        query = f"{primary_keyword} -is:retweet lang:en"
        
        self.logger.info(f"ğŸ¯ çˆ¬å– {category}ï¼ŒæŸ¥è©¢: {query}")
        
        try:
            response = self.client.search_recent_tweets(
                query=query,
                tweet_fields=['created_at', 'author_id', 'public_metrics'],
                user_fields=['username', 'verified'],
                expansions=['author_id'],
                max_results=max_results
            )
            
            if not response or not response.data:
                self.logger.warning(f"âš ï¸ {category}: ç„¡æ¨æ–‡çµæœ")
                return []
            
            # è™•ç†ç”¨æˆ¶ä¿¡æ¯
            users = {}
            if hasattr(response, 'includes') and response.includes and 'users' in response.includes:
                users = {user.id: user for user in response.includes['users']}
            
            # è™•ç†æ¨æ–‡
            for tweet in response.data:
                author = users.get(tweet.author_id)
                
                # è¨ˆç®—äº’å‹•åˆ†æ•¸
                metrics = tweet.public_metrics or {}
                engagement_score = (
                    metrics.get('like_count', 0) * 1 +
                    metrics.get('retweet_count', 0) * 2 +
                    metrics.get('reply_count', 0) * 0.5
                )
                
                tweet_data = {
                    'category': category,
                    'tweet_id': tweet.id,
                    'text': tweet.text,
                    'created_at': tweet.created_at.isoformat() if tweet.created_at else None,
                    'author_id': tweet.author_id,
                    'username': getattr(author, 'username', 'unknown') if author else 'unknown',
                    'verified': getattr(author, 'verified', False) if author else False,
                    'retweet_count': metrics.get('retweet_count', 0),
                    'like_count': metrics.get('like_count', 0),
                    'reply_count': metrics.get('reply_count', 0),
                    'quote_count': metrics.get('quote_count', 0),
                    'engagement_score': engagement_score,
                    'url': f"https://twitter.com/{getattr(author, 'username', 'unknown') if author else 'unknown'}/status/{tweet.id}"
                }
                tweets_data.append(tweet_data)
            
            # æŒ‰äº’å‹•åº¦æ’åº
            tweets_data.sort(key=lambda x: x['engagement_score'], reverse=True)
            
            self.logger.info(f"âœ… {category}: æˆåŠŸç²å¾— {len(tweets_data)} æ¢æ¨æ–‡")
            return tweets_data
            
        except tweepy.TooManyRequests as e:
            self.logger.warning(f"âš ï¸ {category}: Twitter APIé…é¡å·²ç”¨å®Œï¼Œç”Ÿæˆæ¸¬è©¦è³‡æ–™")
            return self.generate_test_data(category)
        except Exception as e:
            self.logger.error(f"âŒ {category}: éŒ¯èª¤ - {str(e)}")
            return []

    def generate_test_data(self, category: str) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¸¬è©¦è³‡æ–™ - ç•¶Twitter APIé…é¡ç”¨å®Œæ™‚ä½¿ç”¨"""
        
        test_tweets = []
        current_time = datetime.now()
        
        # æ ¹æ“šä¸åŒè³½é“ç”Ÿæˆç›¸æ‡‰çš„æ¸¬è©¦å…§å®¹
        test_content = {
            "DeFi": [
                "ğŸš€ DeFiç¸½é–å€‰é‡å‰µæ–°é«˜ï¼å»ä¸­å¿ƒåŒ–é‡‘èæ­£åœ¨é‡å¡‘å‚³çµ±é‡‘èé«”ç³»",
                "ğŸ’° æ–°çš„æµå‹•æ€§æŒ–ç¤¦æ©Ÿæœƒï¼šAPYé«˜é”15%ï¼Œé¢¨éšªæ§åˆ¶è‰¯å¥½",
                "ğŸ”¥ è·¨éˆæ©‹æŠ€è¡“çªç ´ï¼Œå¤šéˆDeFiç”Ÿæ…‹å³å°‡çˆ†ç™¼"
            ],
            "Layer1_Layer2": [
                "âš¡ Ethereum 2.0è³ªæŠ¼é‡çªç ´æ–°ç´€éŒ„ï¼Œç¶²è·¯å®‰å…¨æ€§å¤§å¹…æå‡",
                "ğŸŒŸ Solanaç”Ÿæ…‹é …ç›®æ•¸é‡æ¿€å¢ï¼Œé–‹ç™¼è€…æ´»èºåº¦å‰µæ­·å²æ–°é«˜", 
                "ğŸš€ Layer2æ“´å®¹æ–¹æ¡ˆæ•ˆæœé¡¯è‘—ï¼Œäº¤æ˜“æˆæœ¬é™ä½90%"
            ],
            "NFT_GameFi": [
                "ğŸ® GameFié …ç›®ç”¨æˆ¶æ•¸çªç ´ç™¾è¬ï¼Œé‚Šç©é‚Šè³ºæ¨¡å¼å—åˆ°ç†±æ§",
                "ğŸ–¼ï¸ è—ç±ŒNFTåœ°æ¿åƒ¹ç©©å®šä¸Šå‡ï¼Œå¸‚å ´ä¿¡å¿ƒé€æ¼¸æ¢å¾©",
                "ğŸ”¥ å…ƒå®‡å®™åœŸåœ°äº¤æ˜“æ´»èºï¼Œè™›æ“¬æˆ¿åœ°ç”¢æŠ•è³‡æˆæ–°è¶¨å‹¢"
            ],
            "AI_Crypto": [
                "ğŸ¤– AIèˆ‡å€å¡Šéˆçµåˆç”¢ç”ŸåŒ–å­¸åæ‡‰ï¼Œæ™ºèƒ½åˆç´„è‡ªå‹•å„ªåŒ–æˆç‚ºå¯èƒ½",
                "ğŸ’¡ å»ä¸­å¿ƒåŒ–AIè¨ˆç®—ç¶²è·¯å•Ÿå‹•ï¼Œç®—åŠ›å…±äº«ç¶“æ¿Ÿå³å°‡åˆ°ä¾†",
                "ğŸš€ AIé©…å‹•çš„DeFiç­–ç•¥è¡¨ç¾å„ªç•°ï¼Œè‡ªå‹•åŒ–æŠ•è³‡å›å ±ç‡æå‡"
            ],
            "RWA": [
                "ğŸ¢ ç¾å¯¦ä¸–ç•Œè³‡ç”¢ä»£å¹£åŒ–åŠ é€Ÿï¼Œæˆ¿åœ°ç”¢NFTå¸‚å ´å‡æº«",
                "ğŸ’ é»ƒé‡‘ä»£å¹£åŒ–ç”¢å“å—åˆ°æ©Ÿæ§‹æŠ•è³‡è€…é’ç",
                "ğŸ“ˆ å‚³çµ±é‡‘èå·¨é ­é€²è»RWAé ˜åŸŸï¼Œè³‡ç”¢ä»£å¹£åŒ–æˆä¸»æµ"
            ],
            "Meme_Coins": [
                "ğŸ• DOGEç¤¾ç¾¤æ´»å‹•ç«ç†±ï¼Œé¦¬æ–¯å…‹å†æ¬¡ç‚ºç‹—ç‹—å¹£ç«™å°",
                "ğŸ¸ æ–°èˆˆMemeå¹£ç•°è»çªèµ·ï¼Œç¤¾ç¾¤é©…å‹•åŠ›é‡ä¸å®¹å°è¦·",
                "ğŸš€ Memeå¹£å¸‚å€¼å‰µæ–°é«˜ï¼Œå¨›æ¨‚æ€§æŠ•è³‡æˆå¹´è¼•äººæ–°å¯µ"
            ],
            "Infrastructure": [
                "ğŸ”— Chainlinké è¨€æ©Ÿç¶²è·¯æ“´å±•ï¼Œç‚ºæ›´å¤šDeFié …ç›®æä¾›å¯é æ•¸æ“š",
                "ğŸŒ‰ è·¨éˆåŸºç¤è¨­æ–½å®Œå–„ï¼Œå¤šéˆäº’æ“ä½œæ€§å¤§å¹…æ”¹å–„",
                "âš¡ Web3åŸºç¤è¨­æ–½æŠ•è³‡æ¿€å¢ï¼Œå»ä¸­å¿ƒåŒ–ç¶²è·¯å»ºè¨­åŠ é€Ÿ"
            ]
        }
        
        # ç‚ºç•¶å‰è³½é“ç”Ÿæˆ3æ¢æ¸¬è©¦æ¨æ–‡
        category_content = test_content.get(category, ["ğŸ“Š Web3å¸‚å ´å‹•æ…‹ï¼šå‰µæ–°æŠ€è¡“æŒçºŒæ¨å‹•è¡Œæ¥­ç™¼å±•"])
        
        for i, content in enumerate(category_content[:3]):
            tweet_data = {
                'category': category,
                'tweet_id': f"test_{category}_{i+1}_{int(current_time.timestamp())}",
                'text': content,
                'created_at': (current_time - timedelta(hours=i)).isoformat(),
                'author_id': f"test_user_{i+1}",
                'username': f"Web3News_Test{i+1}",
                'verified': True,
                'retweet_count': 50 + i * 20,
                'like_count': 200 + i * 50,
                'reply_count': 10 + i * 5,
                'quote_count': 5 + i * 2,
                'engagement_score': 300 + i * 75,
                'url': f"https://twitter.com/Web3News_Test{i+1}/status/test_{category}_{i+1}_{int(current_time.timestamp())}"
            }
            test_tweets.append(tweet_data)
        
        # æŒ‰äº’å‹•åº¦æ’åº
        test_tweets.sort(key=lambda x: x['engagement_score'], reverse=True)
        
        self.logger.info(f"âœ… {category}: å·²ç”Ÿæˆ {len(test_tweets)} æ¢æ¸¬è©¦æ¨æ–‡")
        print(f"ğŸ§ª æ¸¬è©¦è³‡æ–™ç”Ÿæˆ: {category} - {len(test_tweets)} æ¢æ¨æ–‡")
        return test_tweets

    def run_daily_crawl(self) -> Dict[str, List[Dict[str, Any]]]:
        """åŸ·è¡Œæ¯æ—¥è¼ªæ›¿çˆ¬å–"""
        
        self.logger.info("ğŸš€ é–‹å§‹è¼ªæ›¿å¼Web3çˆ¬å–...")
        
        # é¸æ“‡ä»Šæ—¥è³½é“
        todays_categories = self.get_todays_categories()
        
        all_tweets = {}
        total_crawled = 0
        
        for i, category in enumerate(todays_categories):
            self.logger.info(f"ğŸ“Š è™•ç† {category} ({i+1}/{len(todays_categories)})...")
            
            keywords = self.web3_categories[category]
            tweets = self.crawl_single_category(category, keywords, max_results=50)
            
            all_tweets[category] = tweets
            total_crawled += len(tweets)
            
            # é¡åˆ¥é–“å»¶é² - å¦‚æœä¸æ˜¯æœ€å¾Œä¸€å€‹
            if i < len(todays_categories) - 1 and tweets:  # åªæœ‰æˆåŠŸæ‰å»¶é²
                delay_minutes = 5  # 5åˆ†é˜å»¶é²
                self.logger.info(f"â° æˆåŠŸçˆ¬å–ï¼Œç­‰å¾… {delay_minutes} åˆ†é˜å¾Œè™•ç†ä¸‹ä¸€å€‹è³½é“...")
                time.sleep(delay_minutes * 60)
        
        # ç‚ºæœªçˆ¬å–çš„è³½é“å¡«å…¥ç©ºé™£åˆ—ï¼ˆä¿æŒçµæ§‹å®Œæ•´ï¼‰
        for category in self.web3_categories.keys():
            if category not in all_tweets:
                all_tweets[category] = []
        
        self.logger.info(f"ğŸ‰ ä»Šæ—¥è¼ªæ›¿çˆ¬å–å®Œæˆï¼")
        self.logger.info(f"ğŸ“ˆ çˆ¬å–è³½é“: {', '.join([k for k, v in all_tweets.items() if v])}")
        self.logger.info(f"ğŸ“Š ç¸½æ¨æ–‡æ•¸: {total_crawled}")
        
        return all_tweets

    def save_results(self, data: Dict[str, List[Dict[str, Any]]]):
        """ä¿å­˜çµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON
        json_filename = f"rotational_web3_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # CSV
        all_tweets = []
        for category, tweets in data.items():
            all_tweets.extend(tweets)
        
        if all_tweets:
            all_tweets.sort(key=lambda x: x.get('engagement_score', 0), reverse=True)
            csv_filename = f"rotational_web3_{timestamp}.csv"
            
            with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=all_tweets[0].keys())
                writer.writeheader()
                writer.writerows(all_tweets)
        
        self.logger.info(f"ğŸ’¾ çµæœå·²ä¿å­˜: {json_filename}")

def main():
    BEARER_TOKEN = os.getenv('TWITTER_BEARER_TOKEN', "AAAAAAAAAAAAAAAAAAAAAF833wEAAAAAVK2bhuSiu%2FaikoUWzmEQvdS%2BJhE%3DjNPAILRXsZOyy1waEYDjahABCRLjG8d9LLyLMAF0CQ3LCckCPq")
    
    print("ğŸ”„ è¼ªæ›¿å¼Web3çˆ¬èŸ²")
    print("=" * 50)
    print("ğŸ¯ ç­–ç•¥: æ¯æ—¥çˆ¬2å€‹è³½é“ï¼Œä¸€é€±è¦†è“‹æ‰€æœ‰è³½é“")
    print("âš¡ å„ªå‹¢: é¿å…APIé™åˆ¶ï¼Œç¢ºä¿æˆåŠŸç‡")
    print("ğŸ§ª è¨»ï¼šTwitter APIé…é¡å·²ç”¨å®Œï¼Œ10/1å‰å°‡ç™¼é€æ¸¬è©¦è¨Šæ¯")
    print("=" * 50)
    
    crawler = RotationalWeb3Crawler(BEARER_TOKEN)
    
    # åŸ·è¡Œçˆ¬å–
    results = crawler.run_daily_crawl()
    
    # ä¿å­˜çµæœ
    crawler.save_results(results)
    
    # é¡¯ç¤ºçµæœæ‘˜è¦
    print("\nğŸ“Š ä»Šæ—¥çˆ¬å–æ‘˜è¦:")
    successful_categories = 0
    total_tweets = 0
    
    for category, tweets in results.items():
        if tweets:
            successful_categories += 1
            total_tweets += len(tweets)
            avg_engagement = sum(t.get('engagement_score', 0) for t in tweets) / len(tweets)
            print(f"   âœ… {category}: {len(tweets)} æ¢æ¨æ–‡ (å¹³å‡äº’å‹•: {avg_engagement:.1f})")
    
    print(f"\nğŸ‰ æˆåŠŸçˆ¬å– {successful_categories} å€‹è³½é“ï¼Œå…± {total_tweets} æ¢æ¨æ–‡")
    print(f"ğŸ” Debug: successful_categories = {successful_categories}, NEWS_REPORTER_AVAILABLE = {NEWS_REPORTER_AVAILABLE}")
    
    # LINE Bot æ¨æ’­åŠŸèƒ½
    if successful_categories >= 1 and NEWS_REPORTER_AVAILABLE:
        try:
            print("\nğŸ“± é–‹å§‹ç”Ÿæˆä¸¦æ¨æ’­ LINE æ–°èå ±å‘Š...")
            
            # ç²å–ç’°å¢ƒè®Šæ•¸
            openai_key = os.getenv('OPENAI_API_KEY')
            line_token = os.getenv('LINE_CHANNEL_ACCESS_TOKEN')
            line_user_id = os.getenv('LINE_USER_ID')
            
            if openai_key and line_token and line_user_id:
                # åˆå§‹åŒ–æ–°èå ±å‘Šå™¨
                reporter = Web3NewsReporter(
                    openai_api_key=openai_key,
                    line_access_token=line_token,
                    line_user_id=line_user_id
                )
                
                # ç”Ÿæˆä¸¦ç™¼é€å ±å‘Š
                success = reporter.generate_and_send_report(results)
                if success:
                    print("âœ… LINE æ–°èå ±å‘Šå·²æˆåŠŸæ¨æ’­ï¼")
                else:
                    print("âŒ LINE æ–°èå ±å‘Šæ¨æ’­å¤±æ•—")
            else:
                print("âš ï¸ ç¼ºå°‘ API è¨­å®šï¼Œè·³é LINE æ¨æ’­")
                
        except Exception as e:
            print(f"âŒ LINE æ¨æ’­éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    
    if successful_categories >= 1:
        print("âœ… è¼ªæ›¿ç­–ç•¥æˆåŠŸï¼å»ºè­°æ¯æ—¥åŸ·è¡Œä»¥å¯¦ç¾å®Œæ•´è¦†è“‹")
    else:
        print("âŒ ä»Šæ—¥çˆ¬å–å¤±æ•—ï¼Œå¯èƒ½éœ€è¦ç­‰å¾…æ›´é•·æ™‚é–“")

if __name__ == "__main__":
    main()