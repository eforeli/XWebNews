#!/usr/bin/env python3
"""
è¼ªæ›¿å¼Web3çˆ¬èŸ² - è§£æ±ºå¤šè³½é“å•é¡Œçš„æœ€çµ‚æ–¹æ¡ˆ
æ¯æ—¥åªçˆ¬2-3å€‹è³½é“ï¼Œè¼ªæ›¿é€²è¡Œï¼Œç¢ºä¿æ‰€æœ‰è³½é“éƒ½æœ‰è¦†è“‹
"""

import tweepy
import json
import time
import csv
from datetime import datetime, timedelta
from typing import List, Dict, Any
import logging
import os
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# å°å…¥ LINE Bot å ±å‘Šå™¨
try:
    from news_reporter import Web3NewsReporter
    NEWS_REPORTER_AVAILABLE = True
except ImportError:
    NEWS_REPORTER_AVAILABLE = False

class RotationalWeb3Crawler:
    def __init__(self, bearer_token: str):
        """è¼ªæ›¿å¼çˆ¬èŸ² - æ™ºèƒ½é¸æ“‡ä»Šæ—¥è¦çˆ¬çš„è³½é“"""
        self.client = tweepy.Client(bearer_token=bearer_token)
        self.setup_logging()
        
        # 7å€‹è³½é“çš„è¼ªæ›¿è¨ˆåŠƒ
        self.web3_categories = {
            "DeFi": ["DeFi", "Uniswap", "compound"],
            "Layer1_Layer2": ["Ethereum", "Solana", "Polygon"], 
            "NFT_GameFi": ["NFT", "OpenSea", "gaming"],
            "AI_Crypto": ["AI", "ChatGPT", "artificial intelligence"],
            "RWA": ["RWA", "tokenization", "BlackRock"],
            "Meme_Coins": ["DOGE", "SHIB", "PEPE"],
            "Infrastructure": ["Chainlink", "oracle", "bridge"]
        }
        
        # è¼ªæ›¿ç‹€æ…‹æª”æ¡ˆ
        self.rotation_file = "crawler_rotation_state.json"

    def setup_logging(self):
        """è¨­ç½®æ—¥èªŒ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('rotational_crawler.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    def get_todays_categories(self) -> List[str]:
        """æ™ºèƒ½é¸æ“‡ä»Šæ—¥è¦çˆ¬çš„è³½é“"""
        
        # è®€å–è¼ªæ›¿ç‹€æ…‹
        if os.path.exists(self.rotation_file):
            try:
                with open(self.rotation_file, 'r') as f:
                    rotation_state = json.load(f)
            except:
                rotation_state = {"last_crawled": {}, "rotation_index": 0}
        else:
            rotation_state = {"last_crawled": {}, "rotation_index": 0}
        
        # æ‰€æœ‰è³½é“
        all_categories = list(self.web3_categories.keys())
        
        # ä»Šæ—¥æ—¥æœŸ
        today = datetime.now().strftime("%Y-%m-%d")
        
        # è¼ªæ›¿é‚è¼¯ï¼šæ¯æ—¥çˆ¬2å€‹è³½é“ï¼Œç¢ºä¿ä¸€é€±å…§è¦†è“‹æ‰€æœ‰7å€‹è³½é“
        start_index = rotation_state.get("rotation_index", 0)
        todays_categories = []
        
        # é¸æ“‡2å€‹è³½é“
        for i in range(2):
            category_index = (start_index + i) % len(all_categories)
            todays_categories.append(all_categories[category_index])
        
        # æ›´æ–°è¼ªæ›¿ç‹€æ…‹
        rotation_state["rotation_index"] = (start_index + 2) % len(all_categories)
        rotation_state["last_crawled"][today] = todays_categories
        
        # ä¿å­˜ç‹€æ…‹
        with open(self.rotation_file, 'w') as f:
            json.dump(rotation_state, f, indent=2)
        
        self.logger.info(f"ğŸ“… ä»Šæ—¥é¸å®šè³½é“: {', '.join(todays_categories)}")
        return todays_categories

    def crawl_single_category(self, category: str, keywords: List[str], max_results: int = 40) -> List[Dict[str, Any]]:
        """çˆ¬å–å–®ä¸€è³½é“ - ä½¿ç”¨æœ€ç°¡å–®ç­–ç•¥"""
        
        tweets_data = []
        
        # ä½¿ç”¨æœ€ç°¡å–®çš„é—œéµå­—
        primary_keyword = keywords[0]
        query = f"{primary_keyword} -is:retweet lang:en"
        
        self.logger.info(f"ğŸ¯ çˆ¬å– {category}ï¼ŒæŸ¥è©¢: {query}")
        
        try:
            response = self.client.search_recent_tweets(
                query=query,
                tweet_fields=['created_at', 'author_id', 'public_metrics'],
                user_fields=['username', 'verified'],
                expansions=['author_id'],
                max_results=max_results
            )
            
            if not response or not response.data:
                self.logger.warning(f"âš ï¸ {category}: ç„¡æ¨æ–‡çµæœ")
                return []
            
            # è™•ç†ç”¨æˆ¶ä¿¡æ¯
            users = {}
            if hasattr(response, 'includes') and response.includes and 'users' in response.includes:
                users = {user.id: user for user in response.includes['users']}
            
            # è™•ç†æ¨æ–‡
            for tweet in response.data:
                author = users.get(tweet.author_id)
                
                # è¨ˆç®—äº’å‹•åˆ†æ•¸
                metrics = tweet.public_metrics or {}
                engagement_score = (
                    metrics.get('like_count', 0) * 1 +
                    metrics.get('retweet_count', 0) * 2 +
                    metrics.get('reply_count', 0) * 0.5
                )
                
                tweet_data = {
                    'category': category,
                    'tweet_id': tweet.id,
                    'text': tweet.text,
                    'created_at': tweet.created_at.isoformat() if tweet.created_at else None,
                    'author_id': tweet.author_id,
                    'username': getattr(author, 'username', 'unknown') if author else 'unknown',
                    'verified': getattr(author, 'verified', False) if author else False,
                    'retweet_count': metrics.get('retweet_count', 0),
                    'like_count': metrics.get('like_count', 0),
                    'reply_count': metrics.get('reply_count', 0),
                    'quote_count': metrics.get('quote_count', 0),
                    'engagement_score': engagement_score,
                    'url': f"https://twitter.com/{getattr(author, 'username', 'unknown') if author else 'unknown'}/status/{tweet.id}"
                }
                tweets_data.append(tweet_data)
            
            # æŒ‰äº’å‹•åº¦æ’åº
            tweets_data.sort(key=lambda x: x['engagement_score'], reverse=True)
            
            self.logger.info(f"âœ… {category}: æˆåŠŸç²å¾— {len(tweets_data)} æ¢æ¨æ–‡")
            return tweets_data
            
        except tweepy.TooManyRequests as e:
            self.logger.error(f"âŒ {category}: APIé™åˆ¶ï¼Œä»Šæ—¥ç„¡æ³•çˆ¬å–æ­¤è³½é“")
            return []
        except Exception as e:
            self.logger.error(f"âŒ {category}: éŒ¯èª¤ - {str(e)}")
            return []

    def run_daily_crawl(self) -> Dict[str, List[Dict[str, Any]]]:
        """åŸ·è¡Œæ¯æ—¥è¼ªæ›¿çˆ¬å–"""
        
        self.logger.info("ğŸš€ é–‹å§‹è¼ªæ›¿å¼Web3çˆ¬å–...")
        
        # é¸æ“‡ä»Šæ—¥è³½é“
        todays_categories = self.get_todays_categories()
        
        all_tweets = {}
        total_crawled = 0
        
        for i, category in enumerate(todays_categories):
            self.logger.info(f"ğŸ“Š è™•ç† {category} ({i+1}/{len(todays_categories)})...")
            
            keywords = self.web3_categories[category]
            tweets = self.crawl_single_category(category, keywords, max_results=50)
            
            all_tweets[category] = tweets
            total_crawled += len(tweets)
            
            # é¡åˆ¥é–“å»¶é² - å¦‚æœä¸æ˜¯æœ€å¾Œä¸€å€‹
            if i < len(todays_categories) - 1 and tweets:  # åªæœ‰æˆåŠŸæ‰å»¶é²
                delay_minutes = 5  # 5åˆ†é˜å»¶é²
                self.logger.info(f"â° æˆåŠŸçˆ¬å–ï¼Œç­‰å¾… {delay_minutes} åˆ†é˜å¾Œè™•ç†ä¸‹ä¸€å€‹è³½é“...")
                time.sleep(delay_minutes * 60)
        
        # ç‚ºæœªçˆ¬å–çš„è³½é“å¡«å…¥ç©ºé™£åˆ—ï¼ˆä¿æŒçµæ§‹å®Œæ•´ï¼‰
        for category in self.web3_categories.keys():
            if category not in all_tweets:
                all_tweets[category] = []
        
        self.logger.info(f"ğŸ‰ ä»Šæ—¥è¼ªæ›¿çˆ¬å–å®Œæˆï¼")
        self.logger.info(f"ğŸ“ˆ çˆ¬å–è³½é“: {', '.join([k for k, v in all_tweets.items() if v])}")
        self.logger.info(f"ğŸ“Š ç¸½æ¨æ–‡æ•¸: {total_crawled}")
        
        return all_tweets

    def save_results(self, data: Dict[str, List[Dict[str, Any]]]):
        """ä¿å­˜çµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON
        json_filename = f"rotational_web3_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # CSV
        all_tweets = []
        for category, tweets in data.items():
            all_tweets.extend(tweets)
        
        if all_tweets:
            all_tweets.sort(key=lambda x: x.get('engagement_score', 0), reverse=True)
            csv_filename = f"rotational_web3_{timestamp}.csv"
            
            with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=all_tweets[0].keys())
                writer.writeheader()
                writer.writerows(all_tweets)
        
        self.logger.info(f"ğŸ’¾ çµæœå·²ä¿å­˜: {json_filename}")

def main():
    BEARER_TOKEN = os.getenv('TWITTER_BEARER_TOKEN', "AAAAAAAAAAAAAAAAAAAAAF833wEAAAAAVK2bhuSiu%2FaikoUWzmEQvdS%2BJhE%3DjNPAILRXsZOyy1waEYDjahABCRLjG8d9LLyLMAF0CQ3LCckCPq")
    
    print("ğŸ”„ è¼ªæ›¿å¼Web3çˆ¬èŸ²")
    print("=" * 50)
    print("ğŸ¯ ç­–ç•¥: æ¯æ—¥çˆ¬2å€‹è³½é“ï¼Œä¸€é€±è¦†è“‹æ‰€æœ‰è³½é“")
    print("âš¡ å„ªå‹¢: é¿å…APIé™åˆ¶ï¼Œç¢ºä¿æˆåŠŸç‡")
    print("=" * 50)
    
    crawler = RotationalWeb3Crawler(BEARER_TOKEN)
    
    # åŸ·è¡Œçˆ¬å–
    results = crawler.run_daily_crawl()
    
    # ä¿å­˜çµæœ
    crawler.save_results(results)
    
    # é¡¯ç¤ºçµæœæ‘˜è¦
    print("\nğŸ“Š ä»Šæ—¥çˆ¬å–æ‘˜è¦:")
    successful_categories = 0
    total_tweets = 0
    
    for category, tweets in results.items():
        if tweets:
            successful_categories += 1
            total_tweets += len(tweets)
            avg_engagement = sum(t.get('engagement_score', 0) for t in tweets) / len(tweets)
            print(f"   âœ… {category}: {len(tweets)} æ¢æ¨æ–‡ (å¹³å‡äº’å‹•: {avg_engagement:.1f})")
    
    print(f"\nğŸ‰ æˆåŠŸçˆ¬å– {successful_categories} å€‹è³½é“ï¼Œå…± {total_tweets} æ¢æ¨æ–‡")
    
    # LINE Bot æ¨æ’­åŠŸèƒ½
    if successful_categories >= 1 and NEWS_REPORTER_AVAILABLE:
        try:
            print("\nğŸ“± é–‹å§‹ç”Ÿæˆä¸¦æ¨æ’­ LINE æ–°èå ±å‘Š...")
            
            # ç²å–ç’°å¢ƒè®Šæ•¸
            openai_key = os.getenv('OPENAI_API_KEY')
            line_token = os.getenv('LINE_CHANNEL_ACCESS_TOKEN')
            line_user_id = os.getenv('LINE_USER_ID')
            
            if openai_key and line_token and line_user_id:
                # åˆå§‹åŒ–æ–°èå ±å‘Šå™¨
                reporter = Web3NewsReporter(
                    openai_api_key=openai_key,
                    line_access_token=line_token,
                    line_user_id=line_user_id
                )
                
                # ç”Ÿæˆä¸¦ç™¼é€å ±å‘Š
                success = reporter.generate_and_send_report(results)
                if success:
                    print("âœ… LINE æ–°èå ±å‘Šå·²æˆåŠŸæ¨æ’­ï¼")
                else:
                    print("âŒ LINE æ–°èå ±å‘Šæ¨æ’­å¤±æ•—")
            else:
                print("âš ï¸ ç¼ºå°‘ API è¨­å®šï¼Œè·³é LINE æ¨æ’­")
                
        except Exception as e:
            print(f"âŒ LINE æ¨æ’­éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    
    if successful_categories >= 1:
        print("âœ… è¼ªæ›¿ç­–ç•¥æˆåŠŸï¼å»ºè­°æ¯æ—¥åŸ·è¡Œä»¥å¯¦ç¾å®Œæ•´è¦†è“‹")
    else:
        print("âŒ ä»Šæ—¥çˆ¬å–å¤±æ•—ï¼Œå¯èƒ½éœ€è¦ç­‰å¾…æ›´é•·æ™‚é–“")

if __name__ == "__main__":
    main()